In this project, I fine-tune the Stable Diffusion v1.4 model with Low-Rank Adaptation (LoRA), a parameter efficient technique that keeps the original pretrained weights frozen and injects compact, trainable low-rank matrices, dramatically reducing the number of parameters updated during training. Starting by scraping Impressionist paintings from Wikimedia Commons and converting them into the Hugging Face “datasets” format, I leverage the train_text_to_image_lora.py example from the Diffusers library to perform the fine-tuning. Due to the hardware limitations I use ICM supercomputer.
